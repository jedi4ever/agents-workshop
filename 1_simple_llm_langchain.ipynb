{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 1 - Simple LLM call using langchain\n",
    "This is the basic lesson to show you how to invoke an LLM using langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# We first install the packages\n",
    "%pip install langchain langchain-openai langchain-community --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We install our envirnment variables helper library\n",
    "%pip install python-dotenv --quiet\n",
    "\n",
    "# We load the library and the environment variables\n",
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write me a shell script to list the files in the current directory. \n",
      "Output the script as markdown code block\n"
     ]
    }
   ],
   "source": [
    "# We load the settings\n",
    "import _settings\n",
    "\n",
    "# The challenge prompt is the challenge we want our agents to solve\n",
    "# You can change it in the _settings.py file\n",
    "# Be sure to restart ^^ your kernel after changing the settings\n",
    "challenge_prompt = _settings.CHALLENGE_PROMPT\n",
    "print(challenge_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```\n",
      "#!/bin/bash\n",
      "\n",
      "# This script lists all the files in the current directory\n",
      "\n",
      "# Navigate to the current directory\n",
      "cd \"$(pwd)\"\n",
      "\n",
      "# Loop through all the files in the directory\n",
      "for file in *\n",
      "do\n",
      "    # Check if the file is a regular file\n",
      "    if [[ -f $file ]]\n",
      "    then\n",
      "        # Print the file name\n",
      "        echo \"$file\"\n",
      "    fi\n",
      "done\n",
      "\n",
      "# Output:\n",
      "# File1.txt\n",
      "# File2.jpg\n",
      "# File3.pdf\n",
      "# ...\n",
      "```\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We load the llm language model from _models\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI()\n",
    "\n",
    "# We invoke the language model with the challenge prompt\n",
    "result = llm.invoke(_settings.CHALLENGE_PROMPT)\n",
    "\n",
    "# We print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what is going of the wire to the LLM and back, we add a callback handler that prints this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============\n",
      "[llm][start] - prompts: Write me a shell script to list the files in the current directory. \n",
      "Output the script as markdown code block\n",
      "\n",
      "=============\n",
      "[llm][end] - generation \n",
      "\n",
      "```\n",
      "#!/bin/bash\n",
      "\n",
      "# This script lists all the files in the current directory\n",
      "\n",
      "# Change directory to current directory\n",
      "cd .\n",
      "\n",
      "# Loop through all files in current directory\n",
      "for file in *\n",
      "do\n",
      "    # Check if file is a regular file\n",
      "    if [ -f \"$file\" ]\n",
      "    then\n",
      "        # Print file name\n",
      "        echo \"$file\"\n",
      "    fi\n",
      "done\n",
      "```\n",
      "\n",
      "To run this script, save it as a .sh file and execute it in the terminal using the command `./script.sh`.\n"
     ]
    }
   ],
   "source": [
    "# We import the callback handler from _callbacks\n",
    "from _callbacks import PrettyPrintCallbackHandler\n",
    "callback = PrettyPrintCallbackHandler()\n",
    "\n",
    "# Now we can do the same call\n",
    "# but this time we will use the callback handler\n",
    "# That will print out the intermediate results\n",
    "# We set the streaming parameter to True to see how the callback works\n",
    "\n",
    "# To make sure we always the same result we set the temperature to 0\n",
    "# Think of reducing the creative randomness of the llm\n",
    "llm = OpenAI(streaming=True, temperature=0 , callbacks=[callback])\n",
    "result=llm.invoke(challenge_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "```\n",
      "#!/bin/bash\n",
      "\n",
      "# This script lists all the files in the current directory\n",
      "\n",
      "# Change directory to current directory\n",
      "cd .\n",
      "\n",
      "# Loop through all files in current directory\n",
      "for file in *\n",
      "do\n",
      "    # Check if file is a regular file\n",
      "    if [ -f \"$file\" ]\n",
      "    then\n",
      "        # Print file name\n",
      "        echo \"$file\"\n",
      "    fi\n",
      "done\n",
      "```\n",
      "\n",
      "To run this script, save it as a .sh file and execute it in the terminal using the command `./script.sh`.\n"
     ]
    }
   ],
   "source": [
    "# We print the final result\n",
    "# And get the same result as before\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4\n"
     ]
    }
   ],
   "source": [
    "# Now to avoid we need to this every time in a notebook\n",
    "# We can put this in a function in _models\n",
    "\n",
    "# The default model is set in _settings.py\n",
    "import _models, _settings\n",
    "# We print the model name from settings\n",
    "print(_settings.MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[chat_model][start] - prompts : Write me a shell script to list the files in the current directory. \n",
      "Output the script as markdown code block\n",
      "\n",
      "=============\n",
      "[llm][end] - generation Here is a simple shell script to list the files in the current directory:\n",
      "\n",
      "```bash\n",
      "#!/bin/bash\n",
      "# This script will list the files in the current directory\n",
      "\n",
      "for file in $(ls)\n",
      "do\n",
      "    echo $file\n",
      "done\n",
      "```\n",
      "\n",
      "This script uses a for loop to iterate over the output of the `ls` command, which lists the files in the current directory. For each file, it uses the `echo` command to print the file name to the console.\n",
      "content='Here is a simple shell script to list the files in the current directory:\\n\\n```bash\\n#!/bin/bash\\n# This script will list the files in the current directory\\n\\nfor file in $(ls)\\ndo\\n    echo $file\\ndone\\n```\\n\\nThis script uses a for loop to iterate over the output of the `ls` command, which lists the files in the current directory. For each file, it uses the `echo` command to print the file name to the console.' response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 29, 'total_tokens': 127}, 'model_name': 'gpt-4', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-8feeab5e-2b47-44e1-87dc-2cb1f07aaeda-0'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We pass this model name to the get_llm helper function\n",
    "llm = _models.get_llm(_settings.MODEL_NAME,callbacks=[callback])\n",
    "result=llm.invoke(challenge_prompt)\n",
    "\n",
    "# We print the final result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31m# WARNING: When changing something here , make sure you restart the notebook kernel\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_openai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mollama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOllama\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;32mdef\u001b[0m \u001b[0mget_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreaming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mopenai_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstreaming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;31m#if name == \"ollama\":\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mollama_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOllama\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"llama3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mstreaming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mollama_llm\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai_llm\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# For reference you can see the whole helper _models.py file\n",
    "%pycat _models.py "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop-agents-DUG816Iy-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
